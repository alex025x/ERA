{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Feature Engineering Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "- Assess target imbalance and decide how to handle it.\n",
        "- Transform the data to prepare for the classification model.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- outputs/datasets/cleaned/TrainSetCleaned.csv\n",
        "- outputs/datasets/cleaned/TestSetCleaned.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "- List of feature engineering steps to add to the ML pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6h-QeuCa2_U"
      },
      "source": [
        "Load the cleaned datasets from the previous data cleaning notebook to begin feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2ELZj83tF1g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned train and test sets\n",
        "train_set_df = pd.read_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\")\n",
        "test_set_df = pd.read_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\")\n",
        "\n",
        "# Convert 'Attrition' to dummy variables\n",
        "train_set_df = pd.get_dummies(train_set_df, columns=['Attrition'], drop_first=True)\n",
        "test_set_df = pd.get_dummies(test_set_df, columns=['Attrition'], drop_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Target Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to ensure the classification model doesn't overfit or underfit the data, the train set needs to be assessed for target imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualize the target variable distribution\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.countplot(x=train_set_df[\"Attrition_Yes\"])\n",
        "plt.show()\n",
        "\n",
        "# Check the balance of the target variable\n",
        "train_set_df[\"Attrition_Yes\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result:\n",
        "\n",
        "We can assess whether there is a significant imbalance in the data. If the imbalance is severe, techniques like oversampling or undersampling might be considered. For now, we continue without balancing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Transformation: Discretization\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "Transform numerical features that may have a skewed distribution or outliers. For instance, YearsAtCompany might have a skewed distribution, so we’ll consider discretizing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from feature_engine.discretisation import ArbitraryDiscretiser\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Discretize YearsAtCompany into categories\n",
        "map_dict = {0: \"≤ 5 years\", 1: \"5-10 years\", 2: \"> 10 years\"}\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('arbd', ArbitraryDiscretiser(binning_dict={'YearsAtCompany': [-np.inf, 5, 10, np.inf]}))\n",
        "])\n",
        "\n",
        "# Apply to train set\n",
        "train_set_df = pipeline.fit_transform(train_set_df)\n",
        "train_set_df[\"YearsAtCompany\"] = train_set_df[\"YearsAtCompany\"].replace(to_replace=map_dict)\n",
        "train_set_df.head(3)\n",
        "\n",
        "# Apply to test set\n",
        "test_set_df = pipeline.fit_transform(test_set_df)\n",
        "test_set_df[\"YearsAtCompany\"] = test_set_df[\"YearsAtCompany\"].replace(to_replace=map_dict)\n",
        "test_set_df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The YearsAtCompany feature is now discretized, potentially improving model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Load custom function provided by CodeInstitute for Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import warnings\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\n",
        "    - Used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape.\n",
        "    - Once transformed, use a reporting tool to evaluate distributions.\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical', 'ordinal_encoder', 'outlier_winsorizer']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\" Check analysis type \"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\" Set suffix columns according to analysis_type\"\"\"\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        list_column_transformers = ['iqr']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "    if analysis_type == 'numerical':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_CategoricalEncoder(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "\n",
        "        if analysis_type != 'ordinal_encoder':\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        else:\n",
        "            if col == column:\n",
        "                DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "            else:\n",
        "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.countplot(data=df_feat_eng, x=col, palette=[\n",
        "                  '#432371'], order=df_feat_eng[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    sns.boxplot(x=df[variable], ax=axes[2])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    axes[2].set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(encoding_method='arbitrary', variables=[\n",
        "                                 f\"{column}_ordinal_encoder\"])\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    try:\n",
        "        disc = Winsorizer(\n",
        "            capping_method='iqr', tail='both', fold=1.5, variables=[f\"{column}_iqr\"])\n",
        "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_iqr\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will encode categorical variables to make them suitable for machine learning models. This includes variables like JobRole, MaritalStatus, and Gender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "# Select categorical variables\n",
        "categorical_variables = train_set_df.select_dtypes(include=[\"object\"]).columns.to_list()\n",
        "\n",
        "# Apply ordinal encoding\n",
        "encoder = OrdinalEncoder(encoding_method=\"arbitrary\", variables=categorical_variables)\n",
        "train_set_df = encoder.fit_transform(train_set_df)\n",
        "test_set_df = encoder.fit_transform(test_set_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Categorical variables are now encoded, making them ready for use in machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numerical Transformation\n",
        "\n",
        "Evaluate and apply any necessary transformations to numerical features to normalize distributions or reduce the impact of outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_variables = train_set_df.drop(\"Attrition_Yes\", axis=1).columns.to_list()\n",
        "numerical_variables = [i for i in all_variables if i not in categorical_variables]\n",
        "\n",
        "# Use the custom FeatureEngineeringAnalysis function to assess the transformations\n",
        "df_engineering = train_set_df[numerical_variables].copy()\n",
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type=\"numerical\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The applied transformations are evaluated for their impact. If no significant improvements are found, we may decide not to apply further transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Smart Correlated Feature Selection\n",
        "\n",
        "Remove highly correlated features to simplify the model and avoid multicollinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "\n",
        "df_engineering = train_set_df.drop(\"Attrition_Yes\", axis=1).copy()\n",
        "\n",
        "spearman_corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.6, selection_method=\"variance\")\n",
        "spearman_corr_sel.fit_transform(df_engineering)\n",
        "spearman_corr_sel.correlated_feature_sets_\n",
        "\n",
        "pearson_corr_sel = SmartCorrelatedSelection(variables=None, method=\"pearson\", threshold=0.6, selection_method=\"variance\")\n",
        "pearson_corr_sel.fit_transform(df_engineering)\n",
        "pearson_corr_sel.correlated_feature_sets_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identify and remove any redundant features that may be highly correlated. If no features are found, all features will be retained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary:\n",
        "\n",
        "The following steps were taken during feature engineering:\n",
        "\n",
        "- Discretization was applied to YearsAtCompany.\n",
        "\n",
        "- Ordinal encoding was applied to categorical features (JobRole, MaritalStatus, etc.).\n",
        "\n",
        "- Numerical transformations were evaluated but not applied as they did not significantly improve distributions.\n",
        "\n",
        "- No features were dropped following the correlated feature analysis.\n",
        "\n",
        "The discretisation and ordinal encoding steps will be added to the ML pipeline."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
